{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model in the AID and UCM dataset\n",
    "\n",
    "This notebook is used to train the final model in the AID and UCM datasets using the pretrained backbone in the HRRS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import modules and define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone repository with the backbone model\n",
    "!git clone https://github.com/LucasKirsten/InceptionV4_TensorFlow2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import sys\n",
    "sys.path.append('./InceptionV4_TensorFlow2')\n",
    "from inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as KL\n",
    "from keras.engine import data_adapter\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mixed precision training (i.e., variables in float16 and computation in float32)\n",
    "# this speeds up the training and allows higher batch sizes\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define strategy (single or multi gpu)\n",
    "if len(tf.config.experimental.list_physical_devices('GPU'))>1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Using mirrored strategy')\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters for training\n",
    "BATCH_SIZE = 64\n",
    "PATH_TFRECORDS = '../input/aid-dataset/AID_tfrecords/*.tfrec'\n",
    "LR = 0.001\n",
    "LR_ATTENUATION = 0.1\n",
    "TOTAL_STEPS = int(9600) # divide by 5 to UCM\n",
    "STEPS_PER_EPOCH = int(1920) # divide by 5 to UCM\n",
    "EPOCHS = int(TOTAL_STEPS/STEPS_PER_EPOCH)\n",
    "NUM_CLASSES = 30 # 30 for AID and 21 for UCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the DL model\n",
    "def get_model():\n",
    "    with strategy.scope():\n",
    "        inputs = KL.Input((256,256,3))\n",
    "        backbone = InceptionResNetV2()\n",
    "        # load pretrained weights\n",
    "        backbone.load_weights('../input/hrrs-pretrained/weights')\n",
    "\n",
    "        x = backbone(inputs)\n",
    "        x = KL.GlobalAveragePooling2D() (x)\n",
    "        x = KL.Dropout(0.2) (x)\n",
    "        x = KL.Dense(NUM_CLASSES, activation='softmax') (x)\n",
    "\n",
    "        model = tf.keras.Model(inputs, x)\n",
    "\n",
    "        # compile model\n",
    "        model.compile(tf.keras.optimizers.SGD(learning_rate=LR), \\ # Adam for better results\n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "        return backbone, model\n",
    "    \n",
    "get_model()[1].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read data from tfrecords\n",
    "def _read_data(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = tf.reshape(tf.io.decode_raw(example['image'], tf.uint8), (256,256,3))\n",
    "    image = tf.cast(image, 'float32')/255.\n",
    "    label = tf.reshape(tf.io.decode_raw(example['label'], tf.float32), (NUM_CLASSES,))\n",
    "    return image, label\n",
    "\n",
    "def get_dataset(paths_tf_records, repeat=True):\n",
    "    with tf.device('/cpu:0'):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "\n",
    "        dataset = tf.data.TFRecordDataset(paths_tf_records,\\\n",
    "                                          compression_type='GZIP',\\\n",
    "                                          num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.shuffle(2048)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(_read_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.batch(BATCH_SIZE)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "def _schedule(epoch, lr):\n",
    "    return lr*LR_ATTENUATION\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(_schedule, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "accs,kappas = [],[]\n",
    "for i in range(5):\n",
    "    # split into train and test\n",
    "    train,test = train_test_split(glob(PATH_TFRECORDS), test_size=0.2)\n",
    "    \n",
    "    # get reset model\n",
    "    backbone, model = get_model()\n",
    "    \n",
    "    # freeze backbone in first epoch\n",
    "    backbone.trainable = False\n",
    "    model.fit(\n",
    "        get_dataset(train),\n",
    "        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "        epochs = 1,\n",
    "        validation_data = get_dataset(test),\n",
    "        validation_steps = len(test)//BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # unfreeze backbone\n",
    "    backbone.trainable = True\n",
    "    model.fit(\n",
    "        get_dataset(train),\n",
    "        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "        epochs = EPOCHS - 1,\n",
    "        callbacks = [lr_scheduler],\n",
    "        validation_data = get_dataset(test),\n",
    "        validation_steps = len(test)//BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # evaluate model and store results\n",
    "    test = get_dataset(test, repeat=False)\n",
    "    y_true,y_pred = [],[]\n",
    "    for x,y in iter(test):\n",
    "        y_pred.append(model(x))\n",
    "        y_true.append(y)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    \n",
    "    y_true,y_pred = np.argmax(y_true,axis=-1),np.argmax(y_pred,axis=-1)\n",
    "    \n",
    "    accs.append(accuracy_score(y_true, y_pred))\n",
    "    kappas.append(cohen_kappa_score(y_true, y_pred))\n",
    "    \n",
    "    print(f'Accuracy: {accs[-1]}')\n",
    "    print(f'Kappa: {kappas[-1]}')\n",
    "    \n",
    "    # delete models of this iteration\n",
    "    del backbone, model\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {np.mean(accs)}±{np.std(accs)}')\n",
    "print(f'Kappa: {np.mean(kappas)}±{np.std(kappas)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "| Optimizer | Model | Accuracy | Kappa |\n",
    "| - | - | - | - |\n",
    "| SGD  | AID | 90.42±0.10 | 0.90±0.001 |\n",
    "| SGD  | UCM | 90.55±2.8  | 0.90±0.029 |\n",
    "| Adam | UCM | 95.77±0.84 | 0.96±0.01 |\n",
    "| Adam | AID | 93.38±0.27 | 0.93±0.003 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
